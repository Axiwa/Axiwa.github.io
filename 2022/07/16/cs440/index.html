<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="google-site-verification" content="" />
  <meta name="referrer" content="unsafe-url">
  
  <title>CS440 Rendering Engine</title>
  <meta name="author" content="Axiwa">
  <meta name="description" content="Silent site">
  
  
  <meta property="og:title" content="CS440 Rendering Engine"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:site_name" content="Azure"/>
  <link href="/apple-touch-icon-precomposed.png" sizes="180x180" rel="apple-touch-icon-precomposed">
  <link rel="alternate" href="/atom.xml" title="Azure" type="application/atom+xml">
  <link rel="stylesheet" href="/css/m.min.css">
  <link rel="icon" type="image/x-icon" href="/favicon.ico">
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>

<meta name="generator" content="Hexo 5.4.2"></head>

<body>
  <a id="top"></a>
  <div id="main">
    <div class="main-ctnr">
      <div class="behind">
  <a href="/" class="back black-color">
    <svg class="i-close" viewBox="0 0 32 32" width="22" height="22" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="3">
        <path d="M2 30 L30 2 M30 30 L2 2"></path>
    </svg>
  </a>
  
</div>


  <article class="standard post">
    <div class="title">
      
  
    <h1 class="page-title center">
        CS440 Rendering Engine
    </h1>
  


    </div>
    <div class="meta center">
      <time datetime="2022-07-15T23:15:22.834Z" itemprop="datePublished">
  <svg class="i-calendar" viewBox="0 0 32 32" width="16" height="16" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
    <path d="M2 6 L2 30 30 30 30 6 Z M2 15 L30 15 M7 3 L7 9 M13 3 L13 9 M19 3 L19 9 M25 3 L25 9"></path>
  </svg>
  &nbsp;
  2022-07-16
</time>


    
    &nbsp;
    <svg class="i-tag" viewBox="0 0 32 32" width="16" height="16" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <circle cx="24" cy="8" r="2"></circle>
      <path d="M2 18 L18 2 30 2 30 14 14 30 Z"></path>
    </svg>
    &nbsp;
    <a href="/categories/Project/">Project</a>





    </div>
    <hr>
    
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#motivation"><span class="toc-text">Motivation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#final-render"><span class="toc-text">Final Render</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#feature-1-3d-mesh-design"><span class="toc-text">Feature 1 3D Mesh Design</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#feature-2-textures"><span class="toc-text">Feature 2 Textures</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#feature-3-homogeneous-media"><span class="toc-text">Feature 3 Homogeneous media</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#feature-4-object-instancing"><span class="toc-text">Feature 4 Object instancing</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A2%8E%E7%A2%8E%E5%BF%B5"><span class="toc-text">碎碎念</span></a></li></ol>
    
    <div class="picture-container">
      
    </div>
    <p>基于<a target="_blank" rel="noopener" href="https://pbr-book.org/">PBRT</a>的离线渲染引擎，基本框架是<a target="_blank" rel="noopener" href="http://rgl.epfl.ch/software/Nori2">Nori</a>.</p>
<p>What we have done in this course and project is to extend this basic renderer to support more full-fledged physically-based lighting/integrating/bsdf functions...</p>
<p>The technical details in this passage may not be perfect.</p>
<span id="more"></span>
<p>Final project was to choose several features from many choices and realize them. What I chose were modeling using Blender, Texture, Homogeneous participating media and Object instancing.</p>
<h1 id="motivation">Motivation</h1>
<p>A damaged, decaying humanoid machine next to a grassy hillside. I would like to show the loop of death and rebirth using a vibrant scene next to a discarded robot. Butterfly also means rebirth in some culture.</p>
<p>Motivational image: <img src="/images/proposal.jpg" /></p>
<p><em>Tales from the Loop</em> by Simon Stalenhag</p>
<p><img src="/images/sky.png" /></p>
<p><em>Castle in the Sky</em> by Tokuma Shoten</p>
<h1 id="final-render">Final Render</h1>
<p><img src="/images/final.png" /></p>
<h1 id="feature-1-3d-mesh-design">Feature 1 3D Mesh Design</h1>
<p><strong>Description</strong></p>
<p>I used the image from animation to model 3D meshes in Blender and gave them a pose. The mesh that I modeled are the robot and grass, and a tree that is used to generate god ray but not presents in the scene.</p>
<h1 id="feature-2-textures">Feature 2 Textures</h1>
<p><strong>Description</strong></p>
<p>Without textures we cannot present a colorful world. The basic idea is that the texture uses the image's RGB information to give our BSDF a specific albedo. In blender, if we uses a UV mapping to let the object know which part of an image its vertices should be mapped to, it will write the corresponding UV coordinates while exporting. UV coordinates are between <span class="math inline">\([0, 1] \times [0,1]\)</span>, which shows the relative location of a point of an image that each vertex is binded. When we evaluate or sample the scattering behavior at the intersection, the UV coorinates have been loaded and recorded in the texture coordinates of mesh so we use the color at that point in the picture instead of an default albedo.</p>
<p><strong>Design choice</strong></p>
<p>Here we only consider the 2D (u, v) mapping. Since the UV coordinates are actually "portion" between 0 and 1, we need to use this value and the size of the reference picture to get the absolute coordinate in it.</p>
<p><span class="math display">\[
\begin{aligned}
x = u * width \\
y = v * height
\end{aligned}
\]</span></p>
<p>We create a <strong>Texture</strong> class to store the image information, including the name, size and the address of the imported image. The public methods includes the constructor which receives a path and load the corresponding image, and <code>getTexel</code> to access the texel given UV coordinates.</p>
<p>We use the <code>stbi_image</code> library to read the image. <code>stbi_load</code> method receives the path of the file (which we preprocess using <code>getFileResolver()</code> of <code>nori</code>) and the channels we expect, then returns the pointer to the address of the image, actual size and channels of the picture.</p>
<p>Take three channels as an example, the image is stored as an array of "RGBRGBRGB...", each takes size of an 'unsigned char *'. Thus when we want to get RGB at an intersection, we first calculate the index in the image <code>(its.uv[0], its.uv[1]) -&gt; (x, y)</code>, then get the 1D array location <code>ty * width + tx</code>, and times the number of channels to get the actual location in the picture <code>#channel * (ty * width + tx)</code>. Also, for 4 channels images, if we tell <code>stbi_load</code> that we just want 3 channels, it will give us the array of "RGBRGBRGB..." but not "RGBARGBA..."</p>
<p>What needs to be noted is that sRGB and linearRGB are different. So we need to use <a target="_blank" rel="noopener" href="https://www.pbr-book.org/3ed-2018/Texture/Image_Texture">color correction</a></p>
<p><strong>Validation</strong></p>
<p>We use the UV mapping in blender to generate correct texture coordinates and export it, then test it in our renderer.</p>
<ol type="1">
<li>Square picture</li>
</ol>
<p>The upper one is rendered by Blender. The texture picture is a 4-channel "color.png"</p>
<p><img src="/images/text_ref.png" /><img src="/images/text.png" /></p>
<ol start="2" type="1">
<li>Rectengular picture</li>
</ol>
<p>The upper one is rendered by Blender. The texture picture is a 3-channel "blue.jpg"</p>
<p><img src="/images/textrec_ref.png" /><img src="/images/textrec.png" /></p>
<h1 id="feature-3-homogeneous-media">Feature 3 Homogeneous media</h1>
<p><strong>Description</strong></p>
<p>There are many ways to implement homogeneous participating media. We use the simplest one: free path sampling. When travelling in the media, the ray has a probability that continue to travel, or scatter (which means the next interaction happens).</p>
<p>If we don't consider the emission, the volume rendering equation is like this:</p>
<p><span class="math display">\[
\begin{aligned}
L(\mathbf{x}, \vec{\omega}) &amp;=T_{r}\left(\mathbf{x}, \mathbf{x}_{z}\right) L\left(\mathbf{x}_{z}, \vec{\omega}\right) \\
&amp;+\int_{0}^{z} T_{r}\left(\mathbf{x}, \mathbf{x}_{t}\right) \sigma_{s}\left(\mathbf{x}_{t}\right) \int_{S^{2}} f_{p}\left(\mathbf{x}_{t}, \vec{\omega}^{\prime}, \vec{\omega}\right) L_{i}\left(\mathbf{x}_{t}, \vec{\omega}^{\prime}\right) d \vec{\omega}^{\prime} d t
\end{aligned}
\]</span></p>
<p>Where <span class="math inline">\(L\left(\mathbf{x}_{z}, \vec{\omega}\right)\)</span> is the radiance background surface radiance, and the second term is the accumulated in-scattered radiance. We calculate the integral by Monte-Carlo method, where the integral over distance <span class="math inline">\(\int_0^z ... dt\)</span> we use the free path sampling to get the probability of how long the ray travels, and the integral over solid angle <span class="math inline">\(\int_{S^2}...d\omega\)</span> we use the phase function to give the next direction and corresponding probability. I use the graph below to show my path tracer. I assume that there is no emitter inside the medium</p>
<p><img src="/images/homo.jpg" /></p>
<p><strong>Design choice</strong></p>
<p>As the figure shows, we need to consider different situations. I will follow the code logic to explain each of them. In my implementation the ray has a record of the medium it is passing by, and there is only one because I don't consider nested medium.</p>
<p>We have a throughput <span class="math inline">\(\beta\)</span>. The ray begins to find the intersection.</p>
<p>It may hit the emitter (which is not shown here). Like other path tracer, if this ray is directly from the eye or last bounce there is no direct illumination, we add <span class="math inline">\(\beta L_e\)</span>.</p>
<p>If it hits a mesh that does not have BSDF and the normal shows the ray is outside this mesh (black ray (2)), we think it's about to go into the volume, so we change the media of the ray <code>ray.media = its.mesh-&gt;getMedium()</code> and change its source <code>ray.o = its.p</code> and <code>maxt</code> to the intersection, continue to find the intersection. After this process we see it as a new ray. If after this we find the ray does not have <code>media</code> and the intersection (maybe the new intersection) does not have a <code>BSDF</code>, some tricky case happens, e.g. two boundaries are tangent to each other and too close, we just stop.</p>
<p>If we find the ray does not have media (purple ray (1)), we are happy. We just need to use the normal direct illumination and mark the variable <code>direct</code> as <code>true</code>.</p>
<p>However, when we are checking if the shadow ray is blocked in situation (1), we need to consider the medium the ray passes by (green ray (3)). If the ray just travels through it and there is no other mesh in the way, we add the radiance of the emitter <span class="math inline">\(\beta L_e\)</span> multiplied by an attenuation term <span class="math inline">\(T_r\)</span> and the probability <code>pdf</code> that this ray will not scatter (in our homogeneous case, <code>pdf</code> = <span class="math inline">\(e^{-\sigma _t t}\)</span> where <span class="math inline">\(t\)</span> is the distance the ray stays in the medium).</p>
<p>If the ray has media. We need to sample the free path first. If our sampled distance is less than that between the ray source and the intersection, scatter happens in the medium (orange ray (4)). The ray's direction may be changed by the phase function and the throughput is multiplied by <span class="math inline">\(T_r(t)\)</span> * <span class="math inline">\(\sigma_s\)</span> * pf-&gt;eval(<span class="math inline">\(\omega\)</span>) / (pdf_t * pdf_{<span class="math inline">\(\omega&#39;\)</span>}). We can also apply the direct illumination by take the attenuation and the pdf into consideration.</p>
<p>If our sampled distance is greater than the distance between the ray source and the intersection, we hit a mesh in the medium. If the mesh has a BSDF (black ray (2)), we evaluate the BSDF and use it to get the ray for next interaction. We can also consider direct illumination. If the mesh does not has a BSDF but has a Medium, it means we hit the boundary of the medium from inside. We just calculate the attenuation and change the source and the max length of the ray and continue. In this situation we don't change the value of <code>direct</code>, and there is no scattering happens.</p>
<p>We still use Russian Roulette to stop the process.</p>
<p>I created a class called <code>Medium</code> and <code>PhaseFunction</code>. <code>PhaseFunction</code> is just like <code>BSDF</code> class, to sample the new direction and to get the probability by inquiring by a <code>struct PFQueryRecord</code>. It is a child of medium property, and in my case I only registered the "isotropic" phase function. <code>Medium</code> is a child of mesh. The instance of <code>Medium</code> can be created while parsing and the <code>Mesh</code> it belongs to will have a pointer to it. <code>Medium</code> has a method called <code>pdf</code> and I use it to evaluate the probability that a ray can travel how long.</p>
<p>Different kinds of <code>Medium</code> can be registered. I created the homogeneous one for the scene and the empty one for test.</p>
<p><strong>Validation</strong></p>
<ol type="1">
<li>Empty</li>
</ol>
<p>Empty medium is quite simple. Its <span class="math inline">\(T_r\)</span> and <span class="math inline">\(pdf\)</span> is just 1.f and the <code>Sample</code> method just "forward" the ray in its direction.</p>
<p>The empty medium should generate exactly same figure as no medium. In the picture we could see a line that shows the existence of bigger ball. Because my renderer uses the intersection and its mesh's BSDF/Medium pointer to determine the location of the ray, it does not handle the case where the light and the boundary are almost tangent/interleaved mesh and volume very well.</p>
<p><img src="/images/sphere_ref.png" /><img src="/images/sphere_empty.png" /></p>
<ol start="2" type="1">
<li>God ray test</li>
</ol>
<p>The upper is the scene rendered by Blender <code>cycles</code>. Both renderer uses 1024 samples/pixel. There are still some problems regarding how to determine what kind of interaction is, so the wall cannot be too close to the volume. The difference may be because the parameters are different.</p>
<p><img src="/images/room_ref.png" /><img src="/images/room.png" /></p>
<ol start="3" type="1">
<li>Numerical validation?</li>
</ol>
<p>Sadly, I could not find a mathematical way to validate it. Though I think I have considered all the situation, it seems to be a kind of "single scattering" which is an incomplete method so not mathematically perfect.</p>
<h1 id="feature-4-object-instancing">Feature 4 Object instancing</h1>
<p><strong>Description</strong></p>
<p>I need to generate tens of thousands of grass and leaves. Though the mesh is small, thousands of objects and the instance of their BSDF/Medium is really a burden to memory. Therefore, we can use object instancing to give each of these repeated objects a pointer that points to a real object, but they themselves are a new kind of class that only contains a pointer and the transform matrix, and its own bounding box. As for BSDF (it can be independent but in my case it is a nullptr) and vertices position information, it only use those of its parent. By this, we avoid use up the memory. By modifying the exporting script for Blender, it will not export the <code>.obj</code> file for those "children" objects but only their transform.</p>
<p><strong>Design choice</strong></p>
<p>I design a new class inherited from the <code>Mesh</code> class called <code>TransformedMesh</code>, that has all its vertex matrix/uv matrix/BSDF... empty, except for a transform matrix, a pointer to its parent, and a bounding box of its real location. When we want to inquire its member, such as BSDF, or a position of a point, it calls the corresponding method of its parent, and if needed, add a transform to it. The information that needs to be transformed directly includes: <code>surfaceArea</code>, <code>totalArea</code>, <code>getBoundingBox</code>. For methods that need to get the position and ray intersection, we do not change it directly, because we handle it in the accelerating structure.</p>
<p>This new class is a registered <code>NoriObject</code>, which is called <code>tmesh</code>. Each <code>TransformedMesh</code> is a child of <code>WavefrontOBJ</code> it is binded to, the easiest way to establish this relationship is to let the <code>tmesh</code> be children of the <code>obj</code> in the <code>.xml</code> file, and while parsing the file, we can add the parent pointer to child and load the transform matrix of child. Type <code>tmesh</code> can only be child of <code>mesh</code>, otherwise the program will complain about "Segment Fault" when the null pointer of <code>tmesh</code> calls its methods. We also get the bounding box of child at the very first, because building the acceleration structure needs all of the meshes and their bounding boxes. We should use a vector to record each child of an <code>obj</code> Mesh, and when its <code>Scene</code> is adding child, the Mesh should tell the Scene there are more <code>tmesh</code> meshes. To save memory, we empty this vector after that.</p>
<p>When doing ray intersection, it is important to note that we cannot return a temporary reference inside a function, and that's why I need to apply transform in the <code>accel.cpp</code> but not the methods of <code>TransformedMesh</code> itself. The bounding box of each <code>tmesh</code> should be its real position because when we are looking for intersection, we must ensure the <code>ray.maxt</code> is updated correctly. When we are using the <code>RayIntersect()</code> method of <code>Mesh</code> (but not <code>Accel</code>), we should transform the ray to the corresponding ray for the "parent" mesh, so an inverse transform is applied. After finding the intersection, we only need to apply the transform to its intersection position, the distance between ray source and the intersection point, its geometry normal and shading normal. For a normal <code>mesh</code>, the transform will also be applied, but it is an identity transform.</p>
<p>For the Blender part, first I need to design a simple mesh like grass or leaf, and use the "particle system" in Blender to generate tens of thousands of them, use the "weight mode" to adjust the distribution of grass and leaves. These objects have a parent object and Blender records the transform that each of them apply to the origin parent object.</p>
<p>Then I need to change the exporting file for Blender <code>io_nori.py</code> to <code>io_nori_matrix.py</code>, because it's meaningless to export so many meshes that will not be loaded by nori. Thanks to the <code>matrix_world</code> API of <code>bpy</code>, we can use it to get the transform of each "child" object with respect to its parent. To distinguish between "parent" and "child" object, I set the transform of all "parent" objects, i.e. objects that should be exported as type "obj" to be identity (using "Apply all the transform" in Blender), and keep the transform of "child" objects, i.e. objects that should be exported as type "tmesh". While exporting, check if the transform of the objects is identity. If yes, write the ".obj" file, otherwise only write its transform to the ".xml" file. A tricky part here is that the transform matrix should be transformed itself due to the <code>nori</code>'s strange perspective, which has a coordinate of (x, -z, y). From linear algebra we know the transform we need is a <span class="math inline">\(4 \times 4\)</span> matrix</p>
<p><span class="math display">\[
 P = 
\begin{bmatrix}
1 &amp;  &amp;  &amp; \\ 
 &amp;  &amp; -1 &amp; \\ 
 &amp; 1 &amp;  &amp; \\ 
 &amp;  &amp;  &amp; 1
\end{bmatrix} 
\]</span></p>
<p>and we apply it to the matrix we get from <code>matrix_world</code> <span class="math inline">\(T\)</span></p>
<p><span class="math display">\[
P^{\top}TP
\]</span></p>
<p>Then we can get a correct transform matrix, and the export plugin will generate a <code>tmesh</code> object like this</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mesh</span> <span class="attr">type</span>=<span class="string">&quot;tmesh&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">string</span> <span class="attr">name</span>=<span class="string">&quot;filename&quot;</span> <span class="attr">value</span>=<span class="string">&quot;meshes/big.obj&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">transform</span> <span class="attr">name</span>=<span class="string">&quot;toWorld&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">matrix</span> <span class="attr">value</span>=<span class="string">&quot;1.5,0.0,0.0,0.0,0.0,1.5,0.0,0.0,0.0,0.0,1.5,1.0,0.0,0.0,0.0,1.0&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">transform</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mesh</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>I haven't figured out how to make the "tmesh" to be the child of "obj" when exporting, so after generating all the <code>obj</code> and <code>tmesh</code> mesh description, we should manually move the tmesh part into its corrsponding parent, like this:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mesh</span> <span class="attr">type</span>=<span class="string">&quot;obj&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">string</span> <span class="attr">name</span>=<span class="string">&quot;filename&quot;</span> <span class="attr">value</span>=<span class="string">&quot;meshes/small.obj&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bsdf</span> <span class="attr">type</span>=<span class="string">&quot;diffuse&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">&quot;albedo&quot;</span> <span class="attr">value</span>=<span class="string">&quot;0.800000011920929,0.800000011920929,0.800000011920929&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bsdf</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mesh</span> <span class="attr">type</span>=<span class="string">&quot;tmesh&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">string</span> <span class="attr">name</span>=<span class="string">&quot;filename&quot;</span> <span class="attr">value</span>=<span class="string">&quot;meshes/big.obj&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">transform</span> <span class="attr">name</span>=<span class="string">&quot;toWorld&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">matrix</span> <span class="attr">value</span>=<span class="string">&quot;1.5,0.0,0.0,0.0,0.0,1.5,0.0,0.0,0.0,0.0,1.5,1.0,0.0,0.0,0.0,1.0&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">transform</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mesh</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mesh</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>Validation</strong></p>
<ol type="1">
<li>Two objects vs one object with a transformed tmesh</li>
</ol>
<p>In blender the real scence is like this:</p>
<p><img src="/images/sphere_blender.png" /></p>
<p>where the big ball has a transform of scale and displacement in y direction with respect to the small one, and the small ball has applied all the transform (identity matrix).</p>
<p>We use the example above to render using two objects (small.obj &amp; big.obj) or only use the small ball and a transform of it.</p>
<p>We can get the same output</p>
<p><img src="/images/sphere_two.png" /><img src="/images/phere_one.png" /></p>
<p>The actual transform is that the big ball is twice the size of small ball, and then translate 1.5m in the negative direction of the y-axis. However, from the transform matrix we could see the translation is at z axis and changes the sign. We should also note that when apply transform in Blender, the parent object's transform should be applied at first, otherwise the matrix we get is not correct.</p>
<ol start="2" type="1">
<li>Memory usage</li>
</ol>
<p>when rendering my final scene. I have 53000 "grass", 3500 "leave_origin", 10000 "furry", 3000 "shrub_sorrel_01_d", 3000 "shrub_sorrel_01_b", 1000 "Grashalm.002" and other meshes, so the roughly memory should be about 8.3 G only for mesh itself.</p>
<p><img src="/images/instance.png" /></p>
<p>The constructing of BVH uses up to 13.7 GB.</p>
<p><img src="/images/bvh.png" /></p>
<p>From the <code>ps v</code> command, we could see the memory used at constructing stage (11 GB) and rendering stage (4.7 GB):</p>
<p><img src="/images/memory_peak.png" /></p>
<p><img src="/images/memory_render.png" /></p>
<p>Therefore the object instancing works.</p>
<h1 id="碎碎念">碎碎念</h1>
<p>Before this course I didn't know coding and art can be so perfectly integrated. The course is soooooo interesting and the professor &amp; TAs are insightful and helpful. 虽然我在反思，对我来说到底是Blender还是CG中的物理数学概率知识更有意思，否则的话怎么hack point都不做.... Anyway，stay determined, Axiwa! 👩‍💻</p>


  </article>
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <div class="busuanzi center">
    page PV:&nbsp;<span id="busuanzi_value_page_pv"></span>&nbsp;・&nbsp;
    site PV:&nbsp;<span id="busuanzi_value_site_pv"></span>&nbsp;・&nbsp;
    site UV:&nbsp;<span id="busuanzi_value_site_uv"></span>
  </div>


    





    </div>
  </div>
  <footer class="page-footer"><div class="clearfix">
</div>
<div class="right-foot">
    <div class="firstrow">
        <a href="#top" target="_self">
        <svg class="i-caret-right" viewBox="0 0 32 32" width="24" height="24" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="3">
            <path d="M10 30 L26 16 10 2 Z"></path>
        </svg>
        </a>
        © Azure 2020-2022
    </div>
    <div class="secondrow">
        <a target="_blank" rel="noopener" href="https://github.com/gaoryrt/hexo-theme-pln">
        Theme Pln
        </a>
    </div>
</div>
<div class="clearfix">
</div>
</footer>
  <script src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script>
<script src="/js/search.min.js"></script>
<script type="text/javascript">

// disqus scripts


// dropdown scripts
$(".dropdown").click(function(event) {
  var current = $(this);
  event.stopPropagation();
  $(current).children(".dropdown-content")[($(current).children(".dropdown-content").hasClass("open"))?'removeClass':'addClass']("open")
});
$(document).click(function(){
    $(".dropdown-content").removeClass("open");
})

var path = "/search.xml";
searchFunc(path, 'local-search-input', 'local-search-result');

</script>

</body>
</html>
