<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <meta name="description" content="Silent site" />
  

  
  
  
  
  
  
  <title>CS449 notes | Azure</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="0603 updated: https:&#x2F;&#x2F;juejin.cn&#x2F;post&#x2F;6844903553727725582">
<meta property="og:type" content="article">
<meta property="og:title" content="CS449 notes">
<meta property="og:url" content="http://axiwa.github.io/2023/04/02/systemsForDataScience/index.html">
<meta property="og:site_name" content="Azure">
<meta property="og:description" content="0603 updated: https:&#x2F;&#x2F;juejin.cn&#x2F;post&#x2F;6844903553727725582">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://axiwa.github.io/images/cs449.png">
<meta property="og:image" content="http://axiwa.github.io/images/cs449-2.png">
<meta property="og:image" content="http://axiwa.github.io/images/cs449-3.png">
<meta property="og:image" content="http://axiwa.github.io/images/cs449-4.png">
<meta property="article:published_time" content="2023-04-01T16:00:00.000Z">
<meta property="article:modified_time" content="2024-03-07T10:43:23.822Z">
<meta property="article:author" content="Axiwa">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://axiwa.github.io/images/cs449.png">
  
    <link rel="alternative" href="https://axiwa.github.io" title="Azure" type="application/atom+xml">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  

  
  <!-- baidu webmaster push -->
  <script src='//push.zhanzhang.baidu.com/push.js'></script>
<meta name="generator" content="Hexo 5.4.2"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>
<body class="home blog custom-background custom-font-enabled single-author">
  <div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Azure" rel="home">Azure</a>
      </h1>
      
        <h2 class="site-description hitokoto"></h2>
        Silent site
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">MENU</button>
            <a class="assistive-text" href="/#content" title="JUMP TO">JUMP TO/a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives">Archives</a></li>
                
                </ul>
            </div>
    </nav>
</header>

      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-systemsForDataScience" class="post-systemsForDataScience post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      CS449 notes
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://axiwa.github.io/2023/04/02/systemsForDataScience/" data-id="cltiebekg0017uucua1hh1vea" class="leave-reply bdsharebuttonbox" data-cmd="more"></a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>0603 updated: <a target="_blank" rel="noopener" href="https://juejin.cn/post/6844903553727725582" class="uri">https://juejin.cn/post/6844903553727725582</a></p>
<span id="more"></span>
<hr>
<h2 id="lec-1-introduction">Lec 1 Introduction</h2>
<h3 id="scale-and-hyperscale">Scale and Hyperscale</h3>
<p>The ability of an architecture to scale appropriately as increased
demand is added to the system. For a datacenter, scaling means
increasing computing ability, memory, networking infrastructure, storage
resources.</p>
<h3 id="scaling-up-and-scaling-out">Scaling up and scaling out</h3>
<p><strong>Scaling up</strong>: More power &amp; resources is added to
existing machines, scaling <strong>vertically</strong>. Constant time as
resources increase in propotion to increasing data size.</p>
<p><strong>Scaling out</strong>: Adding more machines to spread out the
load, taking advantage of the cumulative capacity of shared resources.
Scaling <strong>horizontally</strong>.</p>
<p><strong>Speed-up:</strong> Proportionally less time as resources
increase for a given amount of data.</p>
<h3 id="data-intensive-applications">Data-intensive applications</h3>
<p>Characterized by the fact that they manipulate huge volume of data,
potentially complex, changing, increasing, unable to be handled by a
single computer and fit in memory.</p>
<h3 id="reliability">Reliability</h3>
<p><strong>Byzantine failures:</strong> Failures that are caused by
arbitrary behaviour of some system component and there is imperfect
information on whether a component has failed.</p>
<p><strong>Redundancy:</strong> Achieve system reliability. Backups,
checkpointing, RAID, RDD lineage, etc.</p>
<h3 id="scalability">Scalability</h3>
<p><strong>Measure performance:</strong> Latency, response time, average
versus percentiles (e.g. 95th), etc. <strong>Tail-latency:</strong> A
small percentage of responses of a service that takes more time that
usual. It still may slow down the entire computation.</p>
<h3 id="clouds">Clouds</h3>
<p><strong>Elasticity:</strong> The ability to automatically adapt the
employed resources to varying workload. The typical infrastructure
choice for elastic systems is Cloud computing.</p>
<h2 id="lec-2-distributed-systems">Lec 2 Distributed systems</h2>
<h3 id="definition">Definition</h3>
<p>A system with multiple components (potentially heterogeneous) located
on different machines that communicate and coordinate actions in order
to <strong>appear as a single coherent system to the user</strong>.
Resources of several machines are aggregated, the system is more
scalable, faster, and reliable, more complex (no global clock,
unpredictable failures of components, highly variable bandwidth, large
latency ...).</p>
<h3 id="properties">Properties</h3>
<p>Robustness</p>
<p>Availability: Are services and data always available to clients?</p>
<p>Scalability</p>
<p>Transparency: Be perceived as a whole system rather than a
collection: <a target="_blank" rel="noopener" href="https://superuser.com/questions/1263299/what-is-transparency-in-distributed-systems-in-simple-words" class="uri">https://superuser.com/questions/1263299/what-is-transparency-in-distributed-systems-in-simple-words</a></p>
<p>Concurrency</p>
<p>Security</p>
<p>Efficiency</p>
<h3 id="centralized-architecture-client-server">Centralized architecture
(client-server)</h3>
<p>One entity (the server) has a global view of the system. Example:
<strong>Web</strong>. Simple, easier to control. Not reliable, lack of
scalability.</p>
<h3 id="fully-distributed-architecture-p2p">Fully distributed
architecture (P2P)</h3>
<ul>
<li><p>All participating entities (nodes) are both clients and servers
and contribute the the system they use. There is no central node that
has global knowledge of the system.</p></li>
<li><p><strong>The core: Overlay Networks</strong>: a virtual network
abstraction specifying the topology of the P2P system and implemented on
top of a physical network. Logical or virtual link between nodes
corresponds to a path, perhaps through many physical links.</p></li>
<li><p>Single point of failure (SPOF) is a part system that if it fails,
will stop the entire system from working.</p></li>
<li><p><strong>Topology</strong>:</p>
<ul>
<li>centralized: Every node is connected to other nodes, as well as a
central sever</li>
<li>fully connected: No central entity</li>
<li>hybrid</li>
<li>Structured P2P. DHT-Based: Topology strictly determined by node IDs.
Not all nodes have complete knowledge of all nodes.</li>
</ul></li>
<li><p><strong>Distibuted Hash Table</strong>:</p>
<ul>
<li>Autonomy and decentralization</li>
<li>Fault tolerance</li>
<li>Scalability Each of nodes maintains a set of links (O(log n)) to
other nodes. The stucture of a DHT includes keyspace, keyspace
partitioning scheme, overlay network.</li>
</ul></li>
</ul>
<p>The identifier of a node is its ID (for instance, a SHA1 hash of the
IP). Each object is assigned a 160-bit long identifier (key). Each node
is responsible for a range of keys by specific algorithm. *
Self-organization</p>
<ul>
<li><strong>Pastry</strong>:
<ul>
<li>Each node is assigned a 128-bit long identifier. Place the node and
the objects in a ring uniformly at random.</li>
<li>Each object is associated to the node whose identifier is the
<strong>closest</strong> to the object identifier.</li>
<li>Despite concurrent node failures, eventual delivery is guaranteed
unless [L/2] nodes with adjacent nodeIds fail simultaneously.</li>
<li><strong>Routing</strong>
<ul>
<li><p>In each routing step, a node normally forwards the message to a
node whose nodeId shares with the key a prefix that is at least one
digit longer than the prefix that the key shares with the present node's
id.</p></li>
<li><p><strong>Routing table</strong>: route(key, msg)</p>
<ul>
<li>routeTable(i, l): nodeId matching the current node identifier up to
level i, with the next digit l (-rest of nodeIds)</li>
<li>Lazily repaired</li>
</ul></li>
<li><p><strong>Leaf set</strong>: 8 or 16 closest numerical neighbors in
the naming space. Closest larger and smaller nodeIds. Leaf set is
aggressively monitored and fixed.</p></li>
<li><p><strong>Neighborhood Set</strong>: contains the nodeIds and IP
addresses of the M nodes that are closest to the local node (according
to the <strong>proximity metric</strong>).</p></li>
<li><p>Algorithm on node A: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">IF (D i s wihin range of leaf set)</span><br><span class="line">  DONE</span><br><span class="line">ELSE</span><br><span class="line">  check in the routing table</span><br><span class="line">  IF (there is entry in the routing table, which shares the longer prefix with D than current node)</span><br><span class="line">    msg is forwarded to that node</span><br><span class="line">ELSE</span><br><span class="line">  msg is forwarded to a node hsaring a prefix with the key at least as long as the local node, and is numerically closer to the key.</span><br></pre></td></tr></table></figure> This procedure always
converges.</p></li>
<li><p><strong>Initialization</strong> of the routing table</p>
<ul>
<li>You have a node A to start from, which is already part of the
system. The new nodeId is X. X asks A to route a special "join" message
with the key equal to X, and Pastry routes the join message to the
existing node Z whose id is numerically closest to X. All nodes along
this path from A to Z send their state table to X. X inspects this
information and initializes its own state tables, then informs any nodes
that need to be aware of its arrival.</li>
<li>A is assumed to be in proximity to the new node X, A's neighborhood
set to initialize X's neighborhood set. Z has the closest existing
nodeId to X, thus its leaf set is the basis for X's leaf set.</li>
<li>Consider the condition where A and X share no common prefix. Then
row 0 of A's routing table (A0) contains the appropriate value for X0,
row 1 of B's (B is the first node encountered from A to Z) routing table
is appropriate for X1, C2 is appropriate entries for X2, and so on.</li>
<li>Total cost is <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.952ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4398.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(1450,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="msub" transform="translate(1935,0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(510,-197.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(533,289) scale(0.707)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></g><g data-mml-node="mi" transform="translate(3121.7,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(4009.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></li>
</ul></li>
<li><p><strong>Departure</strong></p>
<ul>
<li>Failure or explicit departure</li>
<li>Nodes may fail or depart without warning. The failure of a node can
be detected by another node that attempts to contact it during routing
and there is no response. This may not affect the routing of a message,
but a replacement entry must be found to preserve the routing
table.</li>
<li>A node attempts to contact each member of the neighborhood set
periodically to see if it is still alive.</li>
</ul></li>
<li><p><strong>Locolity</strong>: Reducing latency</p>
<ul>
<li>Filling the routing table whenever possible with nodes that are
close geographically and satisfying the constraints of the routing
table. (Populating the routing table)</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h2 id="lec-3-cloud-computing">Lec 3 Cloud Computing</h2>
<h3 id="characteristics">Characteristics</h3>
<p>On-demand service</p>
<p>Ubiquitous network access</p>
<p>Location transparent resource polling</p>
<p>Rapid Elasticity</p>
<p>Measured service with pay per use</p>
<p>(Massive scal; Data-intensive; New programming paradigms...)</p>
<h3 id="flavours">Flavours</h3>
<p>Hardware as a service: your own cluster</p>
<p>Infrastructure as a service: flexible computing and storage
infrastructure in the cloud. IaaS is fully self-service for accessing
and monitoring computers, networking, storage, and other services. <a target="_blank" rel="noopener" href="https://avinetworks.com/glossary/infrastructure-as-a-service-iaas/" class="uri">https://avinetworks.com/glossary/infrastructure-as-a-service-iaas/</a></p>
<p>Platform as a service: provide cloud components to certain software
while being used mainly for applications. All servers, storage, and
networking can be managed by the enterprise or a third-party provider
while the developers can maintain management of the applications.</p>
<p>Software as a service: SaaS utilizes the internet to deliver
applications, which are managed by a third-party vendor, to its
users.</p>
<h3 id="mapreduce">MapReduce</h3>
<p>batch computing * Mapper * Each mapper takes fraction of input
(distibuted file system) * UDF * Output: key/value pair (local file
system)</p>
<ul>
<li>Shuffling
<ul>
<li></li>
</ul></li>
<li>Reducer
<ul>
<li>Fetch files from every mapper and merge</li>
<li>UDF</li>
<li>Operate on all intermediate values associated with the same
intermediate key</li>
</ul></li>
</ul>
<h2 id="lec4-gossip-based-computing">Lec4 Gossip-based computing</h2>
<h3 id="multicastgroup-communication">Multicast/Group communication</h3>
<p>A node of a distributed system wants to send a message to a group of
other nodes in the system.</p>
<p>Problems: 1) the latency of the message delivery 2) the load
balancing among nodes 3) the resilience to failure</p>
<h3 id="multicast-topology">Multicast Topology</h3>
<ul>
<li>Centralized
<ul>
<li>fastest</li>
<li>severe unbalance</li>
</ul></li>
<li>Tree-based (spanning tree)
<ul>
<li>load is relatively evenly balanced</li>
<li>fragile structure where the failure of the parent node will lead to
the entire subtree failure</li>
<li>tree maintaince</li>
</ul></li>
<li>Chain-based
<ul>
<li>worst latency</li>
<li>bset load balancing</li>
<li>extremely fragile</li>
</ul></li>
</ul>
<h3 id="gossip-based-dissemination">Gossip-based dissemination</h3>
<p>Each node spreads a message to a sample of other nodes randomly (e.g.
f (fanout) out of n members of the group). Each node reveiving a msg for
the <strong>first</strong> time forwards it to f nodes chosen uniformly
at random among n. Zr is the number of infected nodes prior to round
r.</p>
<ul>
<li><p>Two approaches: Anti-entropy/gossip</p></li>
<li><p>atomic "infection": p(Zr = n) = p(everyone gets a msg) = <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="4.735ex" height="2.086ex" role="img" focusable="false" viewBox="0 -911.2 2092.8 922.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="TeXAtom" transform="translate(499,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msup" transform="translate(778,0)"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="TeXAtom" transform="translate(499,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(778,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g></g></g></g></g></g></svg></mjx-container></span> if f = log(n)+c. The
broadcast is achieved in O(log(n)) hops. <a target="_blank" rel="noopener" href="http://se.inf.ethz.ch/people/eugster/papers/gossips2.pdf" class="uri">http://se.inf.ethz.ch/people/eugster/papers/gossips2.pdf</a></p></li>
<li><p>Pros&amp;Cons</p>
<ul>
<li>simplicity, emergent structure, convergence, robustness from
replication</li>
<li>overhead, hard to cope with malicious behavior</li>
</ul></li>
<li><p><strong>Propagating information ways</strong></p>
<ul>
<li>Push: once you have a multicase message, start gossping</li>
<li>Pull: Periodically pull a few randomly selected nodes for new
msg</li>
<li>Hybrid: First push, last pull</li>
</ul></li>
<li><p><strong>The peer sampling service</strong></p>
<p>Based on <a target="_blank" rel="noopener" href="https://infoscience.epfl.ch/record/109297/files/all.pdf" class="uri">https://infoscience.epfl.ch/record/109297/files/all.pdf</a></p>
<ul>
<li>An n node system; Each node (identified by IP) may join/leave/fail
at any time;</li>
<li>Each node maintains <strong>a local view of c neighbors</strong>
[IP, freshness];</li>
<li>Each node runs a <strong>passive (reactive)</strong> and an
<strong>active(proactive)</strong> thread.</li>
<li><strong>Node selection</strong> =&gt; create an overlay network
<ul>
<li>Periodically each node selects another node as its peer according to
a function <code>peerSelect()</code>. The sender exchange membership
information with it.</li>
<li>Strategies: Rand/Head/Tail</li>
</ul></li>
<li><strong>Data exchange</strong>
<ul>
<li><code>viewPropagation()</code> function decides how nodes exchange
their membership information, whether push/pull/push-pull</li>
<li><code>viewselection(c, buffer)</code> how view selection is
performed. Select the information to exchange during the gossip
operation.</li>
<li><code>buffer(h)</code> [nodeIP, age] of neighbors and self. (Ignore
the oldest h, c/2 first entries of the local view...)</li>
</ul></li>
<li><strong>Data processing</strong>
<ul>
<li>Once a node receives a buffer, the buffer is appended to its local
view</li>
<li>Which entries and how many entries to be removed (h oldest) and kept
(freshest)</li>
<li>s first items are removed: minimizing the correlation between the
local views of the nodes</li>
<li>Random nodes may be removed</li>
<li>Strategies are determined by h (healer) and s(shuffle):
random/healer/shuffle...</li>
</ul></li>
</ul></li>
<li><p>Existing gossip-based membership system</p>
<ul>
<li>Lpbcast/Newscast/Cyclon</li>
</ul></li>
</ul>
<h2 id="lec7-consistency-models-and-protocols">Lec7 Consistency models
and protocols</h2>
<p><a target="_blank" rel="noopener" href="https://hellokangning.github.io/post/consistency-in-distributed-system/" class="uri">https://hellokangning.github.io/post/consistency-in-distributed-system/</a></p>
<p>Most time and energy cost in computing is due to data movement *
Temporal locality and spatial locality * Locality can be exploited by:
caching the data/prefetching/sequential access/patrtitioning the data
among nodes. Some data structure are more appropriate than others to
exploit locality * Partitioning is not applicable everywhere, e.g. graph
(the smaller the cut, the better. Random graphs is resilient to
parition) * <strong>Consistency model</strong> describes the consistency
contract between an application and a storage system. We need conistency
model because there is data replicated. * <strong>Schedules</strong>:
serial schedule/equivalent schedule/serializable schedule...<a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/types-of-schedules-in-dbms/" class="uri">https://www.geeksforgeeks.org/types-of-schedules-in-dbms/</a></p>
<h3 id="strict-consistency">Strict consistency</h3>
<ul>
<li>Any read on a data item X returns a value corresponding to the
result of the most recent write on X.</li>
</ul>
<h3 id="strong-consistencylinearizability-sequential-consistency">Strong
consistency(Linearizability, Sequential consistency)</h3>
<ul>
<li>A write to a variable does not have to be seen instantaneously, but
must be in some sequential order. All memory operations need to happen
in the program order</li>
<li>May produce non-deterministic results</li>
<li>Impossible to achieve in the presence of a partition (CAP), in an
asynchronous system without assumptions on message delivery latencies
(if I don't reveice response of one node for some time, I assume it has
failed)</li>
</ul>
<h3 id="causal-consistency">Causal consistency</h3>
<ul>
<li>There are constraints only for causally related events. Only write
operations that are causally related need to be seen in the same order
by all processes</li>
<li>Monotonic reads consistency: Successive operation by the same
process will return the same value, or more recent value.</li>
<li>Monotonic writes consistency: Any write happens before other writes
on the same variable by the same processor can be seen by same
processor</li>
<li>Read your writes: The effect of a write operation by a process on
data x will be seen by successive read operation on x by the same
process</li>
<li>Writes follow reads</li>
</ul>
<h3 id="eventual-consistency">Eventual consistency</h3>
<ul>
<li>Lack of simultaneous updates</li>
<li>If <strong>no update</strong> takes a very long time, all replicas
eventually become consistent</li>
</ul>
<h3 id="newer-consistency-models">Newer consistency models</h3>
<ul>
<li><p>Per-key sequential</p></li>
<li><p>CRDT: Data structures for which commutated wrties give same
result</p></li>
<li><p>A <strong>Protocol model</strong> describes the implementations
of a consistency model</p></li>
</ul>
<h3 id="primary-based-protocols-centralized">Primary-based protocols:
centralized</h3>
<ul>
<li>A primary server is responsible for every write operation</li>
<li>single point of failure</li>
<li>Write operation take place on the primary replica</li>
<li>Primary server is more loaded than others</li>
</ul>
<h3 id="replicated-write-protocols">Replicated write protocols</h3>
<ul>
<li>Write operation can be done on multiple replicas</li>
<li>They need to obey some ordering</li>
<li>Updates are sent to each replica in the form of an operation in
order to be executed. All updates need to be performed in the same order
in all replicas</li>
<li>Operation order can be managed by a totally-order multicast protocol
(Lamport Timestamps) or a central coordinator</li>
</ul>
<h3 id="optimistic-reconciliation-protocols">Optimistic reconciliation
protocols</h3>
<ul>
<li>Perform on data without synchronization and hope for the best</li>
<li>Manage consistency a posteriori</li>
</ul>
<h3 id="quorum-based-protocols">Quorum-based protocols</h3>
<ul>
<li>Quorum: a number of nodes (&gt;50%) that contain replicas of a given
key-value pair.</li>
<li>Each client should acquire the permission of multiple servers before
reading or writing a replicated data.</li>
<li>N: total replicas; R: read quorum; W: write quorum;</li>
<li><strong>Constraint</strong>: 1. R+W&gt;N 2. W&gt;N/2: Precluding
concurrent write or write/read of two quorums</li>
<li>After the agreement, changes are applied on the file and <em>a new
version number</em> is assigned to the updated file</li>
<li>Constraint 1 guarantees that a replica will not be read and written
at the same time. Constraint 2 guarantees that a replica will no be
written by 2 process at the same time.</li>
<li>(W=N, R=1) is good for read-heavy workloads. (W=1, R=N)/(w=N/2+1,
R=N/2+1)</li>
</ul>
<h3 id="consensus">Consensus</h3>
<ul>
<li><p>Consensus problem: N processes outputs 0 or 1. How to design a
protocol so that at the end, all variables output are 0 or 1). Goal: to
have all processes decide same value</p></li>
<li><p>Constraint: Validity/Integrity/Non-triviality</p></li>
<li><p>FLP: It is impossible to differentiate a failed process with a
slow one in an asynchronous distributed system.</p></li>
<li><p><strong>Synchronous/Asynchronous</strong> distributed system:</p>
<ul>
<li>Each message is received within bounded time/ No bounds</li>
<li>Drift of each process' local clock has a known bound/ Arbitrary</li>
<li>Each step in a process takes lb &lt; time &lt; ub/ No bounds on
message transmission delays</li>
</ul></li>
<li><p>In synchronous system, consensus is solvable. In the asynchronous
system model, it is not possible. But in practice, it is
possible</p></li>
<li><p><strong>Paxos</strong>:</p>
<p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/uploads/prod/2016/12/paxos-simple-Copy.pdf" class="uri">https://www.microsoft.com/en-us/research/uploads/prod/2016/12/paxos-simple-Copy.pdf</a></p>
<ul>
<li><strong>safety</strong> thanks to the notion of majority, no two
different values will be decided</li>
<li><strong>eventual liveness</strong> there will be eventually a unique
leader able to decide</li>
<li>Each <strong>round</strong> has a unique ballot id. Rounds are
synchronous.</li>
<li>If you are in round j and hear a message from round j+1, abort
everything and move over to round j+1</li>
<li>Each round itself broken into <strong>3 phases</strong>: Leader
selection -&gt; Leader proposes a value, processes ack -&gt; Leader
multicasts final value</li>
</ul></li>
</ul>
<h2 id="lec5-lec6-nosql">Lec5 &amp; Lec6 NoSQL</h2>
<h3 id="three-database-revolutions">Three database revolutions</h3>
<ol type="1">
<li>Navigational model</li>
<li>Relational model SQL
<ul>
<li>Logical data is disconneted from physical information storage</li>
<li>Transactions: sequence of operations</li>
<li>Users submit transactions, and can think of each transaction as
executing by itself, no other users.</li>
<li>Each transaction must leave the system in consistent state.
Concurrency is achieved by the DBMS, interleaving actions of various
transactions</li>
</ul></li>
<li>NoSQL: <strong>large-scale dynamic</strong> distributed
workload</li>
</ol>
<h3 id="acid-guarantees">ACID guarantees</h3>
<ul>
<li>Atomicity: all included statements in a trasaction are either
executed or the whole transaction is aborted without affecting the
database</li>
<li>Consistency: a database is in a consistent state before and after a
transaction</li>
<li>Isolation: transactions can not see uncommited changes in the
database</li>
<li>Durability: Changes are written to the disk before a database
commits a transaction, so that committed data will not be lost through
power failure</li>
</ul>
<h3 id="cap-theorem">CAP Theorem</h3>
<p>In a distributed system, you can not achieve these properties
simultaneously, but <strong>at most two</strong> * Consistency: all
users have the same view of the data * Availability: read/write reliably
and quickly. To improve the availability, store data in more than one
site or node * Partition-tolerence: still function normally during
network partition.</p>
<p><strong>Network partition is essential</strong>. Traditional RDBMS
favors <strong>consistency</strong> while NoSQL favors
<strong>availability</strong></p>
<h3 id="base-properties">BASE properties</h3>
<ul>
<li>Basic Availability</li>
<li>Soft-state: Copies of data may be inconsistent</li>
<li>Eventually consistent</li>
</ul>
<h3 id="key-value-data-model-kvs">Key-value data model (KVS)</h3>
<ul>
<li>A collection of key/value pairs</li>
<li>Distributed dictionary data structure, supporting insert, lookup and
delete</li>
<li>Column-oriented storage: store and process by column, keeping all
the data associated with a field next to each other in memory. Range
searches are fast. <a target="_blank" rel="noopener" href="https://dataschool.com/data-modeling-101/row-vs-column-oriented-databases/" class="uri">https://dataschool.com/data-modeling-101/row-vs-column-oriented-databases/</a></li>
<li>Tables: "Column families" in Cassandra, "Table" in HBase,
"collection" in MongoDB...but unstructured, no schema, no foreign
key.</li>
</ul>
<h3 id="graph-data-model">Graph data model</h3>
<h3 id="cassandra"><strong>Cassandra</strong></h3>
<p><a target="_blank" rel="noopener" href="https://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf" class="uri">https://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://medium.com/jorgeacetozi/cassandra-architecture-and-write-path-anatomy-51e339bcfe0c" class="uri">https://medium.com/jorgeacetozi/cassandra-architecture-and-write-path-anatomy-51e339bcfe0c</a>
&lt;- Super clear!</p>
<p><a target="_blank" rel="noopener" href="https://cassandra.apache.org/doc/latest/architecture/storage_engine.html" class="uri">https://cassandra.apache.org/doc/latest/architecture/storage_engine.html</a>
&lt;- Memtable and SSTable</p>
<p>Distributed storage system, large amount of structured data, no
singel point of failure. * C<strong>AP</strong>: Availability and
partition tolerance * Columnbased NoSQL database, but the lowest level
for key-value pairs is row * Different consistency levels * API:
insert(table, key, row_mutation), get(table, key, columnName),
delete(table, key, columnName) * <strong>Data model</strong>: * Table: a
distributed multi dimensional map indexed by a key * Row: associated to
a unique key * Every operation under a single row key is atomic per
replica * Column: smallest data unit in Cassandra: name-value-timestamp
* Supercolumn: column within a column * Column families: columns are
grouped together into sets called column families. There are super
column families and simple column families * Any column within a column
family is accessed by "column_familiy -&gt; column" and any column
within a super column family can be accessed by "column_family -&gt;
super_column -&gt; column" * Keyspace: A namespace defines data
replication on nodes. A cluster contains one keyspace per node</p>
<h3 id="cassandra-architecture">Cassandra Architecture</h3>
<ul>
<li><strong>Partitioning</strong>
<ul>
<li>All nodes participate in a cluster. They do not share anything</li>
<li>Consistent hashing, typically MD5-128: Each node is assigned a
random value which represents its position on the ring. Each data
identified by a key is also hashed to a position of the ring, and then
assigned to a node by their position (chord-like). To address
heterogeneity, load information is analyzed, have lightly loaded nodes
move on the ring to alleviate heavily loaded nodes, or use virtual
nodes. This node is in charge of this key</li>
<li>Departure or arrival affect only immediate neighbors</li>
<li>Each node is aware of every other node in the system</li>
<li>Partitioner: determines how data is distributed across the nodes in
the cluster.</li>
</ul></li>
<li><strong>Replication</strong>
<ul>
<li>Each data item is replicated at N hosts, where N is the replication
parameter. This achieves high-availability. The node stores keys that
fall within its range, and also replicates these keys at the N-1 nodes
in the ring.</li>
<li>Cassandra system elects a leader among its nodes using
<strong>Zookeeper</strong> (Paxos variant). The leader tells all the
nodes on joining the cluster what ranges they are replicas for and makes
a concerted effort to maintain the invariant that <em>no node is
responsible for more than N-1 ranges in the ring</em></li>
<li>Ranges a node is responsible for is cached locally at each node and
in a fault-tolerant manner inside Zookeeper</li>
<li>Replication strategies: Simple Strategy vs Network Topology</li>
<li>Simple Strategy: RandomPartitioner assigns in a chord-like manner
(clockwisely choosing next N-1 nodes to act as replicas),
ByteOrderedPartitioner ranges of keys to servers</li>
<li>Network Topology Strategy: for multi-datacenter deployments</li>
</ul></li>
<li><strong>Write</strong>
<ul>
<li><p>Writes are atomic at row level</p></li>
<li><p>When a client sends request to a node, the node is the
coordinator. Coordinatore uses partitioner (partition policy) to send
query to one or all replica nodes responsible for key.</p></li>
<li><p>When x replicas respond, the coordinator returns an
acknowledgement to the client.</p></li>
<li><p><strong>Hinted Handoff mechanism</strong>: If a replica is down,
the coordinator writes to all other replicas, and keep the write locally
until down replica comes back up. When all replicas are down, the
coordinator buffers writes for up to a few hours, or throws
OverloadedException</p></li>
</ul></li>
<li><strong>Read</strong> <a target="_blank" rel="noopener" href="https://marikalam.medium.com/study-guide-cassandra-data-consistency-496e5bf9cadb" class="uri">https://marikalam.medium.com/study-guide-cassandra-data-consistency-496e5bf9cadb</a>
<a target="_blank" rel="noopener" href="https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/dml/dmlAboutReads.html" class="uri">https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/dml/dmlAboutReads.html</a>
<ul>
<li>The coordinator sends request to replicas, waits for enough
responses and return the latest value to the client</li>
<li>At a replica, check memtable-&gt; row cache-&gt; bloom filter-&gt;
partition key cache-&gt; compression offset map/partition summary.</li>
<li>Consistency is checked. Once a read is done, the coordinator
compares the data from all remaining replicas for the given key. If they
are consistent, the coordinator sends a write request to the out of date
replica to update the row to most recently value.</li>
<li>Maybe slower than write but still fast</li>
</ul></li>
<li>Delete
<ul>
<li>No item deletion right away. Add a tombstone to the log, and will be
deleted at the point when compaction recognize this</li>
</ul></li>
<li>One ring per datacenter, one per-DC coordinator is elected to
coordinate with other DCs.</li>
</ul>
<h3 id="cassandra-data-structures">Cassandra Data Structures</h3>
<ul>
<li>Partitioning key
<ul>
<li>It helps with determining which node in the cluster the data should
be stored</li>
</ul></li>
<li>Commit log
<ul>
<li>The write request is appended to the commit log in the disk</li>
</ul></li>
<li>Memtable
<ul>
<li>A write-back cache. Cassandra looks up by key</li>
<li>The write request is sent to the memtable (in memory)</li>
<li>When the global memory threshold has been reached or commit log is
full, the data is flushed to a SSTable on disk (sequential writes), and
the data in the commit log is purged, memtable is marked as
flushed.</li>
<li>Can be searched by key</li>
<li>Append-only datastructure</li>
</ul></li>
<li>SSTables-Sorted String Table
<ul>
<li>Index structure called <strong>bloom filter</strong>: <a target="_blank" rel="noopener" href="https://llimllib.github.io/bloomfilter-tutorial/" class="uri">https://llimllib.github.io/bloomfilter-tutorial/</a>
<ul>
<li>Compact way of representing a set of items, checking for existence
in set is cheap</li>
<li>A propabilistic data structure. Maybe false positive, never false
negative</li>
</ul></li>
<li>Compaction
<ul>
<li>merging all the updates associated to a given key periodically
(multiple SSTables into one SSTable). Latest timestamp wins</li>
</ul></li>
</ul></li>
<li>Row cache
<ul>
<li>A memory cache which stores recently read rows (records)</li>
</ul></li>
<li>Partition indexes
<ul>
<li>sorted partition keys mapped to their SSTable offsets. Partition
Indexes are created as part of the SSTable creation and resides on the
disk.</li>
</ul></li>
</ul>
<h3 id="membership----gossip-based">Membership -- Gossip based</h3>
<ul>
<li>Anti-entropy gossip based mechanism</li>
<li>Full membership, everyone knows each other.</li>
<li>Any server in cluster could be the coordinator. Every server needs
to maintain a list of all the other servers.</li>
<li>Nodes keep a heartbeat count for each member and periodically gossip
their membership list to neighbours, and then update it: Consistency of
node network.</li>
<li><strong>Failure detection</strong>:
<ul>
<li>A mechanism by which a node can locally determine if any other node
in the system is up or down. In Cassandra, also used to avoid attempts
to communicate with unreachable nodes.</li>
<li>A modified version of the <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.76ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 778 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="398" d="M56 340Q56 423 86 494T164 610T270 680T388 705Q521 705 621 601T722 341Q722 260 693 191T617 75T510 4T388 -22T267 3T160 74T85 189T56 340ZM610 339Q610 428 590 495T535 598T463 651T384 668Q332 668 289 638T221 566Q168 485 168 339Q168 274 176 235Q189 158 228 105T324 28Q356 16 388 16Q415 16 442 24T501 54T555 111T594 205T610 339ZM223 263V422H263V388H514V422H554V263H514V297H263V263H223Z"></path></g></g></g></svg></mjx-container></span> Accrual Failure Detector: emits a
value indicating the suspicion level whether the monitored node is down.
PHI determines the detection timeout.</li>
<li>Adaptively set the timeout based on underlying network, historical
inter-arrival time and failure behavior</li>
</ul></li>
</ul>
<h3 id="consistency">Consistency</h3>
<p>ANY (fastest) / ALL (strong consistency) / ONE / QUORUM (global)</p>
<h3 id="bootstrapping">Bootstrapping</h3>
<h2 id="lec8-scheduling">Lec8 Scheduling</h2>
<h3 id="framework-and-scheduler">Framework and scheduler</h3>
<p>Collections of anything from development tools to middleware to
database services that manages and runs multiple cloud applications. No
single framework is optimal for all applications, and it's difficult to
run each framework on its dedicated cluster.</p>
<p>Goal of scheduling: 1. Good throughput or response time for
tasks/jobs; 2. High utilization of resources. + Computing scheduling:
Running multiple framework on a single cluster</p>
<h3 id="single-core-scheduling">Single-core scheduling</h3>
<ul>
<li>FIFO: average completion time may be high.</li>
<li>STF: Optimal solution wrt average completion time. Not fairest</li>
<li>Round-Robin: Requires to preempt processes at the end of each period
and save their state to resume them later.</li>
<li>...</li>
</ul>
<p>FIFO and STF are better for batch applications, while Round-Robin is
better for interactive applications.</p>
<h3 id="mesos">Mesos</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/21408890" class="uri">https://zhuanlan.zhihu.com/p/21408890</a> Mesos acts as an
intermediary between the nodes and the framework, to accomodate multiple
frameworks. It decouples the resource allocation and the task
scheduling, but not takes all the requirement and resource as input to
give global optimal scheduling.</p>
<ul>
<li>Global scheduler: can achieve optimal but too complex and to handle
future new framework</li>
<li>Distributed scheduler: Master sends resource offers to frameworks:
vector of available resources (e.g. 1CPU; 1GB). Frameworks select which
offers to accept and which tasks to run</li>
<li><strong>Mesos architecture</strong>
<ul>
<li>Mesos master</li>
<li>Mesos slave: executor</li>
<li>Zookeeper: elect master</li>
<li>Frameworks: Scheduler + Executor</li>
</ul></li>
<li>Master knows where the slaves are and what resources the slave have.
Slaves continuously send status updates about resources to the
master.</li>
<li>Two level scheduling: Master sends resource offer to the scheduler
of the framework (framework send their requirement to the master also).
The framework scheduler selects resources and provides tasks. The
executors of the framework launch tasks.</li>
<li>Highly scalable, easy to customize, small Mesos Codebase</li>
</ul>
<h3 id="resource-allocation">Resource allocation</h3>
<ul>
<li>Max-min (weighted) fairness: first allocate average, then if a user
wants less than its fair share he will get less</li>
<li>Users have no reason to lie to ask for more resources.</li>
<li>For multiple cores: <strong>Dominant Resource Fairness</strong>
<ul>
<li>Equalize dominant shares</li>
<li><img src="/images/cs449.png"></li>
</ul></li>
</ul>
<h2 id="lec9-stream-processing">Lec9 Stream Processing</h2>
<p><img src="/images/cs449-2.png"></p>
<h3 id="problem">Problem</h3>
<p>Disseminate streams of events from various producers to various
consumers. Data stream is unbounded data, broken into a sequence of
individual tuples.</p>
<h3 id="messaging-system">Messaging System</h3>
<ul>
<li>Direct Messaging:
<ul>
<li>Necessary in latency critical applications.</li>
<li>Both consumers and producers have to be online at the same time</li>
<li>Issues: comsumer can crash; Producers may send meesages faster than
the consumers can process</li>
</ul></li>
<li><strong>Pub/sub system</strong>
<ul>
<li>A set of Subscribers/consumers and Publishers/producers</li>
<li>Pub-sub system: manages users subscripions, matches published events
against subscriptions, disseminate events</li>
<li>Expressiveness: Topic-based (application-level
multicast)/Content-based (attributed-based; range queries)</li>
<li><strong>Publisher</strong>: generate event data and publishes
them</li>
<li><strong>Subscriber</strong>: submit their subscriptions and process
the events received</li>
<li><strong>P/S service</strong>: the mediator/broker that filters and
routes events from publishers to interested subscribers</li>
<li>Decoupling in time, space and synchronization</li>
<li>Centralized architecture: one centralized broker,
subscribers/publishers do not need to know each other</li>
<li>Distributed architecture: a set of nodes act as brokers</li>
<li>Decentralized architecture: each node can be pub/sub/broker,
communicating directly, they should have knowledge of each other</li>
</ul></li>
</ul>
<h3 id="key-function">Key function:</h3>
<ul>
<li>Event Filtering &amp; Event routing
<ul>
<li>Topic based: Each event is published to one of the channels.
Subscribers subscribe to a particular channel and receive
<strong>all</strong> events published to the subscribed channel
<ul>
<li>Event routing is heavily-loaded</li>
</ul></li>
<li>Content based: Allowing more expresson in the query. Event
publication by a key/value attribute, and subscriptions specify filters
using an explicit subscription language</li>
</ul></li>
</ul>
<h3 id="kafka">Kafka</h3>
<p>Partitioned log-based message broker, distributed stream processing
software, topic oriented * Partitioned log * A log is an append-only
sequence of records on disk, so ordered. But ordering is only guaranteed
within a partition for a topic * A producer sends message by appending
it to the end of the log * A consumer receives messages by reading the
log sequentially, in the order the messages are sorted in the log. *
Messages in the log can be stored for a while * Logs are partitioned
hosted on different machines * Each partition can be read and written
independently of others * Within each partition, the broker assigns a
monotonically increasing sequence number (offset) to every message * No
ordering guarantee across partitions * Partitioned logs can be
replicated or not on different partition * Partitions of a topic are
replicated: fault-tolerent * One broker is the leader of one partition.
Read and write go through the leader * Zookeeper manages consistency: 1)
detecting the addition and the removal of brokers and consumers; 2)
Keeping trak of the comsumed offset of each partition * Followers
passively replicate the leader for fault tolerance. Once a leader fails,
one of the followers will assume the role of the leader * State and
guarantees * Brokers have no metadata for consumers-producers *
Consumers are responsible for keeping track of offsets * Messages in
queues expire based on the pre-configured time periods * Kafka
guarantees that messages from a single partition are delivered to a
consumer in order<br>
* Kafka only guarantees at-least-once delivery (the client needs to
check for duplicate)</p>
<h3 id="streaming-data">Streaming Data</h3>
<ul>
<li>Processing patterns
<ul>
<li>Batch processing</li>
<li>Micro-batch: set of bounded data from unbounded data</li>
<li>Continuous procesing-based system</li>
</ul></li>
<li>Windowing
<ul>
<li>Window is a buffer associated with an input port to retain
previously received tuples</li>
<li>Tumbling window: fixed length, every event belongs to
<strong>excatly one window</strong> (no-overlapping), based on the
happening time.</li>
<li>Hopping window: fixed length, allowing overlap</li>
<li>Sliding window: supports incremental operation, contains all events
that occur within some interval and remove old events when they
expire.</li>
<li>Session window: No fixed duration. Grouping together all events from
the same user that occur closely in time. Frequently used for web site
analysis (e.g. the clicks of a user in some Web page)</li>
<li>Windowing by processing time/event time (handling out-of-order
events)</li>
</ul></li>
</ul>
<h3 id="streaming-processing-system">Streaming processing system</h3>
<ul>
<li>Processing element (PE): A PE is the basic functional unit in an
application: inputs tuples, applies a function, and outputs tuples</li>
<li>A set of PEs and stream connections, organized into a data flow
graph</li>
<li>PE state:
<ul>
<li>stateless tasks: do not maintain state, process each tuple
independently of prior history: total parallelism, no
synchronization</li>
<li>stateful tasks: maintains information across different tuples to
detect complex patterns</li>
</ul></li>
<li>At runtime, an application is represented by one or more jobs. Jobs
are deployed as a collection of PEs.</li>
<li>Logical plan vs. physical plan: Logical plan is a data flow graph,
where the vertices correspond to PEs, and the edges to stream
connections; Physical plan is a data flow graph where the vertices
correspond to OS process, and the edges to transport connections</li>
<li>Parallelization: pipelined/task/data</li>
<li>How to allocate data items to computation instance: broadcast,
shuffle, key-base...</li>
</ul>
<h2 id="lec10-distributed-learning">Lec10 Distributed learning</h2>
<h3 id="parameter-server">Parameter server</h3>
<p>To launch ML algorithms on a typical Cloud infrastructure, going
beyond typical cluster-compute system</p>
<p>Parameters are stored in a distributed database (e.g. KVS) accesible
through the network.</p>
<ul>
<li>Architecture
<ul>
<li>Server nodes
<ul>
<li>maintain a partition of the globally shared parameters</li>
<li>communicate with each other to replicate or migrate parameters</li>
<li><strong>perform bookeeping and global aggregation
steps</strong></li>
</ul></li>
<li>Worker nodes
<ul>
<li><strong>perform computation</strong></li>
<li>store locally a portion of the training data</li>
<li>communicate with server nodes to update and retrieve the shared
parameters</li>
</ul></li>
</ul></li>
</ul>
<h3 id="distributed-gradient-descent">Distributed Gradient Descent</h3>
<ul>
<li>Workers get the assigned training data</li>
<li>Workers <strong>pull</strong> the working set of model</li>
<li>Different parts of the model may be on different servers</li>
<li>Iterate until Stop:
<ul>
<li>Workers <strong>compute</strong> gradients</li>
<li>Workers <strong>push</strong> gradients</li>
<li>Servers <strong>aggregate</strong> into current model</li>
<li>Workers <strong>pull</strong> updated model</li>
</ul></li>
<li><img src="/images/cs449-3.png"></li>
<li>Idealy, different workers follow the distribution of the data, as
homogeneous as possible. But this will slow down the system</li>
</ul>
<h3 id="shared-parameters-key-value-vectors">Shared parameters:
key-value vectors</h3>
<p>Model parameters are represented as key-value pairs (i, wi), but the
semantics used by the server may be of vector or matrix. This enables to
linear algebra operation and data locality.</p>
<h3 id="range-push-and-pull">Range Push and Pull</h3>
<p>Caller of the push and pull are always the workers. Range of keys can
minimize network traffic. Non blocking operations: the caller inserts
its requests in a queue and resume computation</p>
<h3 id="asynchronous-execution-and-flexible-consistency-model">Asynchronous
execution and Flexible consistency model</h3>
<p>How to deal with inconsistency? <strong>Stale synchronous parallel
(SSP)</strong> * <strong>Bounded delay</strong>: x-bounded delay means
the workerss (at time t) can tolerate which level out-of-date parameters
(<strong>can't be older thatn t-x</strong>). 1-bounded means sequential,
infnity means fully asynchronous. * Delayed SGD: trade-off between
efficiency vs. synchronization * <strong>Vector clocks</strong>:
attached for each k-v pair for: tracking aggregation status, rejecting
doubly sent data, recovery from failure....<a target="_blank" rel="noopener" href="https://lrita.github.io/2018/10/24/lamport-logical-clocks-vector-lock/">What
is vector clock</a>, <a target="_blank" rel="noopener" href="https://lrita.github.io/images/posts/distribution/p558-lamport.pdf">Lamport
timestamp</a> * As many k-v pairs get updated at the same time during
one iteration, they can share the same clock stamps, which reduces the
space requirement * Consistent Hashing &amp; Replication * Use of DHT
range partitioning * Servers are hashed in the ring * Server nodes store
a replica of k-v pairs on k nodes counter clockwise to it.</p>
<h3 id="federated-learning"><strong>Federated Learning</strong></h3>
<p>Let the data stays where it is produced. Model parameters will never
contain more information than the raw training data. Instead of
uploading the raw data, train a model locally and upload the model.</p>
<p>In practice: * Each round, a random fraction C of clients are
selected by the server. Each client that has <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.379ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 1051.4 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(633,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></svg></mjx-container></span> training data samples in federated
learning ~ a randomly selected sample in traditional learning (C=1 means
full-batch gradient descent) * The server sends model to clients,
clients send updates to the server, the server aggregates, then sends
back new parameters * Suppose n samples; K clients; <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.379ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 1051.4 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(633,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></svg></mjx-container></span> data samples on the client k; <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.124ex" height="1.489ex" role="img" focusable="false" viewBox="0 -442 497 658"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D702" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q156 442 175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336V326Q503 302 439 53Q381 -182 377 -189Q364 -216 332 -216Q319 -216 310 -208T299 -186Q299 -177 358 57L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></span> learning rate; central server
broadcasts current model <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.385ex" height="1.359ex" role="img" focusable="false" viewBox="0 -443 1054.3 600.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></svg></mjx-container></span> to the
selected clients, then * training objective: <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="10.082ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4456.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="msub" transform="translate(1223,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(633,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g></g><g data-mml-node="mi" transform="translate(2412.3,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(2962.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(3351.3,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(4067.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span> * <img src="/images/cs449-4.png"> * Each client k computed gradient: <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="13.624ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6021.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(510,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(1206.2,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2262,0)"><path data-c="2207" d="M46 676Q46 679 51 683H781Q786 679 786 676Q786 674 617 326T444 -26Q439 -33 416 -33T388 -26Q385 -22 216 326T46 676ZM697 596Q697 597 445 597T193 596Q195 591 319 336T445 80L697 596Z"></path></g><g data-mml-node="msub" transform="translate(3095,0)"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mi" transform="translate(676,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(4189.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(4578.4,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(5632.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span> * Depends on: *
Fraction of the clients * Number of passes a client makes on its local
data * Local mini-batch size used for the client update</p>
<h2 id="lec11-decentralized-learning">Lec11 Decentralized Learning</h2>
<h2 id="lec1213-graph-mining">Lec12&amp;13 Graph mining</h2>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2023/04/02/systemsForDataScience/">
    <time datetime="2023-04-01T16:00:00.000Z" class="entry-date">
        2023-04-02
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Lecture-notes/">Lecture notes</a>
  </div>

    
    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text"></h3>
    
        <span class="nav-previous"><a href="/2023/04/02/scalaReview/" rel="prev"><span class="meta-nav"></span> Scala notes</a></span>
    
    
        <span class="nav-next"><a href="/2022/12/10/randomNumber/" rel="next"> <span class="meta-nav"></span></a></span>
    
</nav><!-- .nav-single -->







</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Computer-graphics/">Computer graphics</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Debug/">Debug</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Knowledge/">Knowledge</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Lecture-notes/">Lecture notes</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Project/">Project</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%9B%E4%BD%9C/"></a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BA%9F%E8%AF%9D/"></a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%8E%A9/"></a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%B7%91%E5%9B%A2/"></a><span class="category-list-count">1</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2024/03/07/diary/"></a>
          </li>
        
          <li>
            <a href="/2024/01/27/story/"></a>
          </li>
        
          <li>
            <a href="/2023/06/17/coc/">Replay</a>
          </li>
        
          <li>
            <a href="/2023/04/02/VulkanBasic/">Vulkan(1) Basics</a>
          </li>
        
          <li>
            <a href="/2023/04/02/hair/">Hair Rendering</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  
    
  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2024 Axiwa
    All rights reserved.</p>
    <p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></p>
</footer>
    <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":false,"bdPic":"","bdStyle":"2","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/js/share.js'];</script>

<script src="/js/jquery-3.3.1.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>